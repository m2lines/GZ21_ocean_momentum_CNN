#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import gz21_ocean_momentum.common.cli as cli
import gz21_ocean_momentum.common.assorted as common
import gz21_ocean_momentum.common.bounding_box as bounding_box
import gz21_ocean_momentum.unsorted.train_data_xr_to_pytorch as lib
from gz21_ocean_momentum.models import submodels

import configargparse

import os

import xarray as xr
import torch

# TODO probably temporary
import tempfile

# ---

old_imports = """
import os.path
import copy
import argparse
import importlib
import pickle
from dask.diagnostics import ProgressBar
import numpy as np
import mlflow
import xarray as xr

import torch
from torch.utils.data import DataLoader
from torch import optim
from torch.optim.lr_scheduler import MultiStepLR

from gz21_ocean_momentum.train.base import Trainer
from gz21_ocean_momentum.inference.utils import create_test_dataset
from gz21_ocean_momentum.inference.metrics import MSEMetric, MaxMetric
import gz21_ocean_momentum.train.losses

import gz21_ocean_momentum.step.train.lib as lib
from   gz21_ocean_momentum.common.bounding_box import load_bounding_boxes_yaml
"""

# ---

_cli_desc = """
Train a Pytorch neural net to predict subgrid ocean momentum forcing from
ocean surface velocity.

Uses data generated by the GZ21 data step script.
"""

p = configargparse.ArgParser(description=_cli_desc)
p.add("--config-file", is_config_file=True, help="config file path")
p.add("--in-train-data-dir",         type=str,   required=True, help="training data in zarr format, containing ocean velocities and forcings")
p.add("--subdomains-file",           type=str,   required=True, help="YAML file describing subdomains to split input data into (see readme for format)")
p.add("--batch-size",                 type=int,   required=True, help="TODO")
p.add("--epochs",                    type=int,   required=True, help="number of epochs to train for")
p.add("--out-model",                 type=str,   required=True, help="save trained model to this path")
p.add("--initial-learning-rate",     type=float, required=True, help="initial learning rate for optimization algorithm")
p.add("--decay-factor",              type=float, required=True, help="learning rate decay factor, applied each time an epoch milestone is reached")
p.add("--decay-at-epoch-milestones", type=int, action="append", required=True, help="milestones to decay at. May specify multiple times. Must be strictly increasing with no duplicates")
p.add("--device",  type=str, default="cuda", help="neural net device (e.g. cuda, cuda:0, cpu)")
p.add("--weight-decay",              type=float, default=0.0, help="Weight decay parameter for Adam loss function. Deprecated, default 0.")
p.add("--train-split", type=float, required=True, help="0>=x>=1. Use 0->x of input dataset for training")
p.add("--test-split",  type=float, required=True, help="0>=x>=1. Use x->end of input dataset for training. Must be greater than --train-split")
p.add("--printevery", type=int, default=20)
options = p.parse_args()

# TODO raehik 2023-11-13: parse, don't validate
if not common.list_is_strictly_increasing(options.decay_at_epoch_milestones):
    cli.fail(2, "epoch milestones list is not strictly increasing")

torch.autograd.set_detect_anomaly(True)

def _check_dir(dir_path):
    """
    Create directory if it does not already exist.

    Parameters
    ----------
    dir_path : str
        string of directory to check/make
    """
    if not os.path.exists(dir_path):
        os.mkdir(dir_path)

# --------------------------
# SET UP TRAINING PARAMETERS
# --------------------------
# Note that we use two indices for the train/test split. This is because we
# want to avoid the time correlation to play in our favour during test.
model_module_name = "models.models1"
model_cls_name = "FullyCNN"
loss_cls_name = "HeteroskedasticGaussianLossV2"
transformation_cls_name = "SoftPlusTransform"
# Submodel (for instance monthly means)
submodel = "transform3"

# Directories where temporary data will be saved
data_location = tempfile.mkdtemp()
print("Created temporary dir at  ", data_location)

FIGURES_DIRECTORY = "figures"
MODELS_DIRECTORY = "models"
MODEL_OUTPUT_DIR = "model_output"

for directory in [FIGURES_DIRECTORY, MODELS_DIRECTORY, MODEL_OUTPUT_DIR]:
    _check_dir(os.path.join(data_location, directory))

submodel_transform_func = lambda x: submodels.transform3.fit_transform(x)

# load input training data, split into spatial domains via provided bounding
# boxes
ds = xr.open_zarr(options.in_train_data_dir)
f_bound_cm26 = lambda x: bounding_box.bound_dataset("yu_ocean", "xu_ocean", ds, x)
sd_dss_xr = map(f_bound_cm26, bounding_box.load_bounding_boxes_yaml(options.subdomains_file))

# transform shorthand
submodel_transform_and_to_torch = lambda x: lib.gz21_train_data_subdomain_xr_to_torch(submodel_transform_func(x))

datasets = map(submodel_transform_and_to_torch, sd_dss_xr)

train_dataloader, test_dataloader = lib.prep_train_test_dataloaders(
    datasets, options.train_split, options.test_split, options.batch_size)

# -------------------
# LOAD NEURAL NETWORK
# -------------------
# Load the loss class required in the script parameters
n_target_channels = datasets[0].n_targets
criterion = getattr(train.losses, loss_cls_name)(n_target_channels)

# Recover the model's class, based on the corresponding CLI parameters
try:
    models_module = importlib.import_module(model_module_name)
    model_cls = getattr(models_module, model_cls_name)
except ModuleNotFoundError as e:
    raise type(e)("Could not find the specified module for : " + str(e))
except AttributeError as e:
    raise type(e)("Could not find the specified model class: " + str(e))
net = model_cls(datasets[0].n_features, criterion.n_required_channels)
try:
    transformation_cls = getattr(transforms, transformation_cls_name)
    transformation = transformation_cls()
    transformation.indices = criterion.precision_indices
    net.final_transformation = transformation
except AttributeError as e:
    raise type(e)("Could not find the specified transformation class: " + str(e))

print("--------------------")
print(net)
print("--------------------")
print("***")


# Log the text representation of the net into a txt artifact
with open(
    os.path.join(data_location, MODELS_DIRECTORY, "nn_architecture.txt"),
    "w",
    encoding="utf-8",
) as f:
    print("Writing neural net architecture into txt file.")
    f.write(str(net))

# Add transforms required by the model.
for dataset in datasets:
    dataset.add_transforms_from_model(net)


# -------------------
# TRAINING OF NETWORK
# -------------------
# Adam optimizer
# To GPU
net.to(device)

# Optimizer and learning rate scheduler
optimizer = optim.Adam(
        list(net.parameters()),
        lr=options.initial_learning_rate, weight_decay=options.weight_decay)
lr_scheduler = MultiStepLR(
        optimizer, options.decay_at_epoch_milestones,
        gamma=options.decay_factor)

trainer = Trainer(net, device)
trainer.criterion = criterion
trainer.print_loss_every = options.printevery

# metrics saved independently of the training criterion.
metrics = {"R2": MSEMetric(), "Inf Norm": MaxMetric()}
for metric_name, metric in metrics.items():
    metric.inv_transform = lambda x: test_dataset.inverse_transform_target(x)
    trainer.register_metric(metric_name, metric)

for i_epoch in range(options.epochs):
    print(f"Epoch number {i_epoch}.")
    # TODO remove clipping?
    train_loss = trainer.train_for_one_epoch(
        train_dataloader, optimizer, lr_scheduler, clip=1.0
    )
    test = trainer.test(test_dataloader)
    if test == "EARLY_STOPPING":
        print(test)
        break
    test_loss, metrics_results = test
    # Log the training loss
    print("Train loss for this epoch is ", train_loss)
    print("Test loss for this epoch is ", test_loss)

    for metric_name, metric_value in metrics_results.items():
        print(f"Test {metric_name} for this epoch is {metric_value}")
    mlflow.log_metric("train loss", train_loss, i_epoch)
    mlflow.log_metric("test loss", test_loss, i_epoch)
    mlflow.log_metrics(metrics_results)
# Update the logged number of actual training epochs
mlflow.log_param("n_epochs_actual", i_epoch + 1)


# ------------------------------
# SAVE THE TRAINED MODEL TO DISK
# ------------------------------
net.cpu()
torch.save(net.state_dict(), options.out_model)
net.to(device=device)

# Save other parts of the model
# TODO this should not be necessary
#print("Saving other parts of the model")
#full_path = os.path.join(data_location, MODELS_DIRECTORY, "transformation")
#with open(full_path, "wb") as f:
#    pickle.dump(transformation, f)


# ----------
# DEBUT TEST
# ----------
for i_dataset, dataset, test_dataset, xr_dataset in zip(
    range(len(datasets)), datasets, test_datasets, xr_datasets
):
    test_dataloader = DataLoader(
        test_dataset, batch_size=options.batch_size, shuffle=False, drop_last=True
    )
    output_dataset = create_test_dataset(
        net,
        criterion.n_required_channels,
        xr_dataset,
        test_dataset,
        test_dataloader,
        test_index,
        device,
    )

    # Save model output on the test dataset
    output_dataset.to_zarr(
        os.path.join(data_location, MODEL_OUTPUT_DIR, f"test_output{i_dataset}")
    )
